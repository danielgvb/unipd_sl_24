# EDA for statistical learning project------------
# Import data------------------
data <- read_excel("GitHub/unipd_sl_24/data/data.xlsx",sheet = "dataframe_col")
View(data)
# EDA for statistical learning project------------
# Import data------------------
data <- read_excel("GitHub/unipd_sl_24/data/data.xlsx",sheet = "dataframe_col")
# EDA for statistical learning project------------
# Import data------------------
library(readxl)
data <- read_excel("GitHub/unipd_sl_24/data/data.xlsx",sheet = "dataframe_col")
data <- read_excel("../data/data.xlsx",sheet = "dataframe_col")
View(data)
str(data)
head(data)
str(data)
# Clean data-------------------
# Counting NA values in each column
na_counts <- apply(data, 2, function(x) sum(is.na(x)))
# Print the counts of NA values per column
print(na_counts)
# Removing rows with any NA values
clean_data <- na.omit(data)
# View description --------------
# Get the summary of the dataframe
summary_stats <- summary(clean_data)
# Print the summary statistics
print(summary_stats)
data_reduced <- subset(clean_data, select = -c(MY, date))
View(data_reduced)
# Histograms for each variable
par(mfrow = c(3, 5))  # Adjust layout to fit all histograms
for (i in 1:ncol(data_reduced)) {
hist(data_reduced[[i]], main = names(data_reduced)[i], xlab = names(data_reduced)[i], col = "blue", border = "black")
}
par(mfrow = c(1, 1))  # Reset layout
par(mfrow = c(3, 5))  # Adjust layout to fit all histograms
for (i in 1:ncol(data_reduced)) {
hist(data_reduced[[i]], main = names(data_reduced)[i], xlab = names(data_reduced)[i], col = "blue", border = "black")
}
# Histograms
par(mar = c(2, 2, 2, 2))  # Adjust margins to make them smaller
# Adjust layout to fit all histograms
par(mfrow = c(3, 5))  # Example layout; adjust if necessary
# Histograms for each variable
for (i in 1:ncol(data)) {
hist(data[[i]], main = names(data)[i], xlab = names(data)[i], col = "blue", border = "black")
}
# Reset layout
par(mfrow = c(1, 1))
# Histograms
par(mar = c(2, 2, 2, 2))  # Adjust margins to make them smaller
# Adjust layout to fit all histograms
par(mfrow = c(3, 5))  # Example layout; adjust if necessary
# Histograms for each variable
for (i in 1:ncol(data_reduced)) {
hist(data_reduced[[i]], main = names(data_reduced)[i], xlab = names(data_reduced)[i], col = "blue", border = "black")
}
# Reset layout
par(mfrow = c(1, 1))
# Create a pairs plot excluding columns C and D
pairs(data_reduced)
# Basic boxplot for multiple columns
# Setting up the plotting area
par(mfrow = c(1, ncol(data_reduced)))  # Arrange plots in 1 row and as many columns as variables
# Loop over each column to create a boxplot
for (i in 1:ncol(data_reduced)) {
boxplot(data_reduced[, i], main = names(data_reduced)[i], col = rainbow(ncol(data_reduced))[i])
}
# Basic boxplot for multiple columns
# Setting up the plotting area
par(mfrow = c(1, ncol(data_reduced)))  # Arrange plots in 1 row and as many columns as variables
# Loop over each column to create a boxplot
for (i in 1:ncol(data_reduced)) {
boxplot(data_reduced[, i], main = names(data_reduced)[i], col = rainbow(ncol(data_reduced))[i])
}
# Histograms-----------------
par(mar = c(2, 2, 2, 2))  # Adjust margins to make them smaller
# Adjust layout to fit all histograms
par(mfrow = c(3, 5))  # Example layout; adjust if necessary
# Histograms for each variable
for (i in 1:ncol(data_reduced)) {
hist(data_reduced[[i]], main = names(data_reduced)[i], xlab = names(data_reduced)[i], col = "blue", border = "black")
}
# Reset layout
par(mfrow = c(1, 1))
cov_matrix <- cov(data_reduced)
cov_matrix
correl_matrix <- cor(data_reduced)
correl_matrix
# Heatmap using base R
image(1:ncol(correl_matrix), 1:ncol(correl_matrix), correl_matrix,
main = "Correlation Matrix Heatmap",
xlab = "Variables", ylab = "Variables", axes = FALSE,
col = heat.colors(20))
axis(1, at = 1:ncol(correl_matrix), labels = colnames(correl_matrix), las = 2)
axis(2, at = 1:ncol(correl_matrix), labels = colnames(correl_matrix), las = 2)
# Correlation matrix
cor_matrix <- cor(data_reduced, use = "complete.obs")
print(cor_matrix)
# Visualize the correlation matrix
heatmap(cor_matrix, symm = TRUE)
View(data_reduced)
# Variable Transformations----------------
# Create new log-transformed variables
data_reduced <- data_reduced %>%
mutate(across(c(y, industrial_inputs, metals, energy, shipping, fx,
industrial_prod, construction_licences_area, google_trends),
list(log = ~log(.)), .names = "{.col}_log"))
# Variable Transformations----------------
library(dplyr)
# Create new log-transformed variables
data_reduced <- data_reduced %>%
mutate(across(c(y, industrial_inputs, metals, energy, shipping, fx,
industrial_prod, construction_licences_area, google_trends),
list(log = ~log(.)), .names = "{.col}_log"))
# Subset the dataframe by selecting specific columns
data_final <- data_reduced %>%
select(y_log, industrial_inputs_log, metals_log, energy_log,
shipping_log, fx_log,industrial_prod_log,
construction_licences_area_log, google_trends_log,
unemployment, interest_rate)
library(car)
vif(lm(y_log ~ ., data = data_reduced))  # Replace 'Y' with the dependent variable name
library(car)
# Multicolinearity Check--------------------
install.packages("car")
library(car)
vif(lm(y_log ~ ., data = data_reduced))  # Replace 'Y' with the dependent variable name
vif(lm(y_log ~ ., data = data_final))  # Replace 'Y' with the dependent variable name
# Assume var_to_exclude is the name of the variable you want to exclude
var_to_exclude <- "industrial_inputs_log"
# Create a formula that excludes the specific variable
predictors <- setdiff(names(data_final), c("y_log", var_to_exclude))
formula <- as.formula(paste("y_log ~", paste(predictors, collapse = " + ")))
# Fit the model and calculate VIF
vif_values <- vif(lm(formula, data = data_final))
print(vif_values)
# Linear Regression-----------------
# Perform linear regression
model <- lm(y_log ~ ., data = data_final)  # Replace 'Y' with the dependent variable name
summary(model)
# Predict and evaluate the model on the test set
predictions <- predict(model, newdata = data_final)
actuals <- data_final$y_log  # Replace 'Y' with the dependent variable name
# Calculate evaluation metrics
mse <- mean((predictions - actuals)^2)
print(mse)
e     <- residuals(model)
y.hat <- fitted.values(model)
sum(e)
sum(e*x)
# Calculate evaluation metrics
mse <- mean((predictions - actuals)^2)
print(mse)
residuals     <- residuals(model)
y.hat <- fitted.values(model)
sum(residuals)
# Plot QQ plot and histogram for residuals
qqnorm(residuals, main = "QQ Plot of Residuals")
qqline(residuals, col = "red")
# Plot QQ plot and histogram for residuals
qqnorm(residuals, main = "QQ Plot of Residuals", col ='blue')
qqline(residuals, col = "red")
# Do Test of normality to residuals
# Perform Shapiro-Wilk test
shapiro_test <- shapiro.test(residuals)
print(shapiro_test)
# Perform Kolmogorov-Smirnov test
ks_test <- ks.test(residuals, "pnorm", mean = mean(residuals), sd = sd(residuals))
print(ks_test)
# Check for Homoskedasticity
plot(fitted(model), residuals(model), main="Residuals vs Fitted")
abline(h=0, col="red")
# Breusch-Pagan Test
install.packages("lmtest")
library(lmtest)
bptest(model)
# Cook Distance
# Cook's Distance Plot
plot(cooks.distance(model), main="Cook's Distance")
abline(h = 4/(nrow(data_final)-length(coef(model))), col="red")  # Common threshold line
full.mod <- lm(y_log ~ ., data = data_final)
# Leverage Plot
plot(hatvalues(model), main="Leverage Values")
abline(h = 2*mean(hatvalues(model)), col="red")  # Common threshold line
# Necessary libraries
library(readxl)
library(dplyr)
library(car)
library(lmtest)
library(zoo)
library(caret)
#Read the data using the read_excel function
#Change here for current data path
data <- read_excel("C:/Users/CAMILO/Documents/GitHub/unipd_sl_24/data",sheet = "dataframe_col")
#Read the data using the read_excel function
#Change here for current data path
data <- read_excel("C:/Users/CAMILO/Documents/GitHub/unipd_sl_24/data.xlsx",sheet = "dataframe_col")
#Read the data using the read_excel function
#Change here for current data path
data <- read_excel("C:/Users/CAMILO/Documents/GitHub/unipd_sl_24/data.csv",sheet = "dataframe_col")
#Read the data using the read_excel function
#Change here for current data path
data <- read_excel("C:/Users/CAMILO/Documents/GitHub/unipd_sl_24/data/data.csv",sheet = "dataframe_col")
#Read the data using the read_excel function
#Change here for current data path
data <- read_excel("C:/Users/CAMILO/Documents/GitHub/unipd_sl_24/data/data.xlsx",sheet = "dataframe_col")
View(data)
#Present the first 5 rows of the data
head(data,5)
# Counting NA values in each column
na_counts <- apply(data, 2, function(x) sum(is.na(x)))
# Print the counts of NA values per column
print(na_counts)
# Interpolate finished_constructions
# Interpolate NA values in the finished_constructions column
# Interpolate internal NA values in the finished_constructions column
data$finished_constructions <- na.approx(data$finished_constructions, na.rm = FALSE)
# Fill leading NA values with the first non-NA value
data$finished_constructions <- na.locf(data$finished_constructions, na.rm = FALSE, fromLast = FALSE)
# Fill trailing NA values with the last non-NA value
data$finished_constructions <- na.locf(data$finished_constructions, na.rm = FALSE, fromLast = TRUE)
#Read the data using the read_excel function
#Change here for current data path
data <- read_excel("C:/Users/CAMILO/Documents/GitHub/unipd_sl_24/data/data.xlsx",sheet = "dataframe_col")
View(data)
#Present the first 5 rows of the data
head(data,5)
# Counting NA values in each column
na_counts <- apply(data, 2, function(x) sum(is.na(x)))
# Print the counts of NA values per column
print(na_counts)
# Interpolate finished_constructions
# Interpolate NA values in the finished_constructions column
# Interpolate internal NA values in the finished_constructions column
data$finished_constructions <- na.approx(data$finished_constructions, na.rm = FALSE)
# Fill leading NA values with the first non-NA value
data$finished_constructions <- na.locf(data$finished_constructions, na.rm = FALSE, fromLast = FALSE)
# Fill trailing NA values with the last non-NA value
data$finished_constructions <- na.locf(data$finished_constructions, na.rm = FALSE, fromLast = TRUE)
# Fill trailing NA values with the last non-NA value
data$finished_constructions <- na.locf(data$finished_constructions, na.rm = FALSE, fromLast = TRUE)
# Removing rows with any NA values
clean_data <- na.omit(data)
data_reduced <- subset(clean_data, select = -c(MY))
View(data_reduced)
# View description --------------
# Get the summary of the dataframe
summary_stats <- summary(select(data_reduced,-date))
# Print the summary statistics
print(summary_stats)
df_with_z_scores <- data_reduced %>%
mutate(across(where(is.numeric), ~ scale(.), .names = "z_{col}"))
# Identify outliers
outlier_threshold <- 3
df_with_outliers <- df_with_z_scores %>%
mutate(across(starts_with("z_"), ~ abs(.) > outlier_threshold, .names = "outlier_{col}"))
# Print data frame with Z-scores and outliers identified
print(df_with_outliers)
outlier_counts <- df_with_outliers %>%
summarise(across(starts_with("outlier_"), ~ sum(. == TRUE), .names = "count_{col}"))
# Print the counts
outlier_counts
# print the row that has outliers
# Filter the data frame where "outlier_z_energy" is TRUE
filtered_df <- df_with_outliers %>%
filter(outlier_z_energy == TRUE | outlier_z_shipping == TRUE)
# print the row that has outliers
# Filter the data frame where "outlier_z_energy" is TRUE
filtered_df <- df_with_outliers %>%
filter(outlier_z_energy == TRUE | outlier_z_shipping == TRUE)
head(filtered_df)
# Filter original df to see when it was
filtered_df_energy <- clean_data %>%
filter(energy > 370)
filtered_df_energy
# see high values of shipping when happend
filtered_df_shipping <- clean_data %>%
filter(shipping > 468)
filtered_df_shipping
# Create a pairs plot excluding columns C and D
# pairs(data_reduced) # does not say much
```
# Select subset of variables to difference
variables_to_calculate <- c("y", "industrial_inputs", "metals",
"energy", "shipping", "fx", "industrial_prod",
"construction_licences_area", "finished_constructions",
"google_trends")
# Create a function to calculate the 12-month percentual variation
percentual_variation_12_months <- function(x) {
return((x / dplyr::lag(x, n = 12) - 1))
}
# Apply the function to the subset of variables
data_percentual_variation <- data %>%
mutate(across(all_of(variables_to_calculate), percentual_variation_12_months, .names = "pct_var_{col}"))
View(data_percentual_variation)
