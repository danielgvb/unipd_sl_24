---
title: "statistical_project"
output: pdf_document
date: "2024-05-27"
Authors: Daniel Gutierrez, Rafael Hernandez, Camilo Sanabria
---

## Project Introduction

In this project we aim to study the influence of macroeconomics indicators on the house price index in Colombia. This indicator measure the evolution through time of the market prices of residential properties as a percentage change. The idea of the analysis is to present and study the possible effects of the macroeconomics indicators on the house prices through the construction of regression models, evaluating their results and interpreting the models in the context of the data.

## 1. Dataset Introduction and Preprocessing

The dataset was constructed gattering information from multiple sites: the Colombian department of statistics (DANE) , the Colombian central bank (Banco de la Repúlbica), Google trends and the Federal Reserve Economic Data (FRED). In general, all the visited sites present the data as a .csv file with tables according to filters related with the time span of interest. With this we construct a consolidated database un .csv format with the following variables:

| Variable                   | Description                                                                                                    |
|------------------------|------------------------------------------------|
| House Price Index (y)      | Measure of the average change in the residential properties price as an index                                  |
| Industrial_inputs          | Measure of the average changes in the industry input costs as an index                                         |
| metals                     | Measure of the average price change in metal costs excluding gold as an index                                  |
| energy                     | Measure of the average price change of energy including Crude oil, Natural Gas, Coal Price and Propane Indices |
| shipping                   | Measure the average price change of shipping costs                                                             |
| fx                         | Indicator of the foreign exchange rate of the Colombian Peso (COP) with respect of the USD                     |
| unemployment               | Indicator of the percentage of unemployment population in Colombia                                             |
| industrial_prod            | Measure of the level of production on industrial sectors as an index                                           |
| interest_rate              | Reference interest rate emited by the colombian central bank with respect of the others financial institutions |
| construction_licences_area | Measure the total of squared meters given for construction licences in Colombia                                |
| finished_constructions     | Measure of the total constructions that have been finished in Colombia                                         |
| google_trends              | Google search trends on housing for Colombia                                                                   |

These variables present real values,and were selected as they have an initial coherent relation with the housing sector. We present the import process of the data

```{r import-data}
# Necessary libraries to run all the code chunks and plots
library(readxl)
library(dplyr) 
library(car) 
library(lmtest) 
library(zoo)
library(caret)
```

# Data Uploading

```{r}
#Read the data using the read_excel function
#Change here for current data path
data <- read_excel("C:/Users/CAMILO/Documents/GitHub/unipd_sl_24/data/data.xlsx",sheet = "dataframe_col")
View(data)
#Present the first 5 rows of the data
head(data,5)
```

# Data pre-processing and cleaning

After have an initial version of the data we proceed with the pre-processing and cleaning steps. As a first approximation we identify the number of NA values in each one of the columns. We realize that the variable finished_constructions requires an additional pre-processing step because it is recorded by quarters while the other are registered by months. We propose the following reconstruction steps:

```{r}

# Counting NA values in each column
na_counts <- apply(data, 2, function(x) sum(is.na(x)))
# Print the counts of NA values per column
print(na_counts)

# Construction and Industrial prod have nans, 
# they are at the extremes 


# Convert date column to Date type if it's not already
data$date <- as.Date(data$date)

# Copy the dataframe to avoid modifying the original one
data_filled <- data


# Interpolate finished_constructions
# Find the indices where the date is at the end of a trimester
trimester_end_indices <- which(format(data$date, "%m") %in% c("03", "06", "09", "12"))

# Loop through each trimester end and distribute the value to the previous three months
for (i in trimester_end_indices) {
  if (i - 2 > 0) {
    # Distribute the value to the current month and the previous two months
    value_to_distribute <- data$finished_constructions[i] / 3
    data_filled$finished_constructions[i] <- value_to_distribute
    data_filled$finished_constructions[i - 1] <- value_to_distribute
    data_filled$finished_constructions[i - 2] <- value_to_distribute
  }
}

data_filled$finished_constructions <- na.locf(data_filled$finished_constructions, na.rm = FALSE)

#View(data_filled)
```

(Not implemented as the result is no good) As we are interested in use the House Price Index as regressor related with the effect of the other macroeconomic variables, for that reason we implement a lag of one period over this variable

```{r}
# Do lag of the dependent variable bc should be included as regressor
data <- data %>%
  mutate(y_1 = lag(y, n = 1))
```

# Summary stats and final dataset

We proceed to remove the left rows with NA values eliminating from the dataset the registers before January 2005, also we drop the column MY also referred to the date in form of MONTH()YEAR(). Finally, we present a summary of the dataset after these transformations. In particular we see that the Housing Price Index range present values on [87.35,203.50] with an IQR=70.38.

```{r}
# Removing rows with any NA values
clean_data <- na.omit(data)

# Drop columns ------------

data_reduced <- subset(clean_data, select = -c(MY))

View(data_reduced)
# View description --------------
# Get the summary of the dataframe
summary_stats <- summary(select(data_reduced,-date))

# Print the summary statistics
print(summary_stats)
```

## Exploratory Analysis

# Outliers based on z scores (Eliminate?)

As a first step on the exploratory analysis we identify the outliers presented in the data (At this moment this analysis was implemented considering a z score fitting a distribution over the time series, at least they allow to see some outliers like the pick presented in the 2022 due to external effects)

```{r}
# Hacer interquartil range
# see outliers based on z scores

df_with_z_scores <- data_reduced %>%
  mutate(across(where(is.numeric), ~ scale(.), .names = "z_{col}"))

# Identify outliers
outlier_threshold <- 3
df_with_outliers <- df_with_z_scores %>%
  mutate(across(starts_with("z_"), ~ abs(.) > outlier_threshold, .names = "outlier_{col}"))

# Print data frame with Z-scores and outliers identified
print(df_with_outliers)

outlier_counts <- df_with_outliers %>%
  summarise(across(starts_with("outlier_"), ~ sum(. == TRUE), .names = "count_{col}"))

# Print the counts
outlier_counts

# print the row that has outliers
# Filter the data frame where "outlier_z_energy" is TRUE
filtered_df <- df_with_outliers %>%
  filter(outlier_z_energy == TRUE | outlier_z_shipping == TRUE)

head(filtered_df)

# Filter original df to see when it was
filtered_df_energy <- clean_data %>%
  filter(energy > 370)

filtered_df_energy

# see high values of shipping when happend
filtered_df_shipping <- clean_data %>%
  filter(shipping > 468)

filtered_df_shipping

# It was aug 2022-dec 2022, peak inflation wave, war on Ukraine and,
# preocupation of winter in Europe, makes sense to leave it

# Create a pairs plot excluding columns C and D
```

#Outliers based on the IQR

As an initial step to better understand the data and its behavior we find the outliers of each one of the variables using the IQR to select those points outside the bounds. As a result we found that the variables that present the most quantity of outliers are the shipping, the interest rate and the unemployment.

```{r}
# Function to detect outliers based on IQR
detect_outliers_single_var <- function(column) {
  column <- na.omit(column) 
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  outliers <- column[column < lower_bound | column > upper_bound]
  return(outliers)
}

# List to store outliers for each variable
outliers_list <- list()

# Loop through each column in the dataset
for (var in colnames(select(data_reduced,-date))) {
  if (is.numeric(data[[var]])) {
    outliers_list[[var]] <- detect_outliers_single_var(data[[var]])
  }
}

# Print outliers for each variable
print(outliers_list)


```

We present the box plot for the variables with the highest quantity of outliers

```{r}
# Create the boxplot without x-axis labels

{
par(mfrow = c(3, 1),mar = c(5, 5, 4, 2) + 0.1)
boxplot(data_reduced$shipping, horizontal=TRUE  ,frame=F,main = "Boxplot of shipping")

boxplot(data_reduced$unemployment, horizontal=TRUE  ,frame=F,main = "Unmeployment")

boxplot(data_reduced$interest_rate, horizontal=TRUE  ,frame=F,main = "Interest rate")
}
```

#Ploting the Home Price Index

Continuing with the exploratory analysis we plot the variable of Home Price Index to see its behavior through time. It is possible to see a clear increase tendency starting from the year 2005 until the year 2021, then a little decay was recorded.

```{r}
# reset margins
# Reset margins to default values
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow = c(1, 1))

# dependent variable
data_reduced$category <- as.factor(clean_data$date)
# Set up the plot without drawing the points
{plot(1:length(data_reduced$category), clean_data$y, type = "n", xaxt = "n",xlab="date", ylab = "Home Price Index", main = "Evolution of Home Prices over time")

# Add the categorical labels to the x-axis
axis(1, at = 1:length(data_reduced$category), labels = data_reduced$category)

# Add the lines and points to the plot
lines(1:length(data_reduced$category), data_reduced$y, type = "o", col = "blue", pch = 16)}
```

# Ploting percentual variation of the Home Price Index

Another useful information can be found in the box plots and histograms for the percentual variations. We take the percentual variation of the variables with the intention of identify patterns and tendencies in a clearly way, considering that the majority of it is composed by index, and the study of it changes is fundamental for a complete understanding of their behavior throught time. We start with the Home Price Index variable.

In these plots we see that the central value of the percentual variations is positive, indicating and overall increase in the Home Price Index through time. Also the negatives outliers were expected because those negatives variations due to the decrease after the 2021.In the histogram it is possible to see a right skewed behavior.

```{r}
# Create Differences (Inflation)
colnames(data_reduced)

# Select subset of variables to difference
variables_to_calculate <- c("y", "industrial_inputs", "metals",
                            "energy", "shipping", "fx", "industrial_prod",
                            "construction_licences_area", "finished_constructions",
                            "google_trends")


# Create a function to calculate the 12-month percentual variation
percentual_variation_12_months <- function(x) {
  return((x / dplyr::lag(x, n = 12) - 1))
}

# Apply the function to the subset of variables
data_percentual_variation <- data %>%
  mutate(across(all_of(variables_to_calculate), percentual_variation_12_months, .names = "pct_var_{col}"))


View(data_percentual_variation)
```

```{r}
# Histograms/Boxplots of the percentual variations
# Subset only percentual changes
data_percentual_variation <- data_percentual_variation %>%
  select(pct_var_y, pct_var_industrial_inputs, pct_var_metals, pct_var_energy,
         pct_var_shipping, pct_var_fx,pct_var_industrial_prod,
         pct_var_construction_licences_area, pct_var_finished_constructions,
         pct_var_google_trends,
         unemployment, interest_rate)
par(mfrow = c(1, 2))
# Y
{boxplot(data_percentual_variation$pct_var_y , horizontal=TRUE  ,frame=F,main = "Boxplot of Percentual Variation Y")
hist(data_percentual_variation$pct_var_y , breaks=40,main = "Histogram of Percentual Variation Y",xlab = "Percentual Variation")}
```

We did a similar procedure for the rest of the variables with respect to their percentual variation

```{r}

# Histograms-----------------
par(mar = c(2, 2, 2, 2))  # Adjust margins to make them smaller

# Adjust layout to fit all histograms
par(mfrow = c(3, 5))  # Example layout; adjust if necessary

# Histograms for each variable

for (i in 1:ncol(data_percentual_variation)) {
  hist(data_percentual_variation[[i]], main = names(data_percentual_variation)[i], xlab = names(data_percentual_variation)[i], col = "blue", border = "black")
}

```

For an easier interpretation we plot the boxplot for the percentual variation of each variable in a same graph. This plot shows that the index variables with a high variability in percentual variation are the shipping index price and the energy index price. Also, the index variable that show less variability in the percentual variation is the one related with the industrial production.

```{r}
# Reset layout
par(mfrow = c(1, 1))

pairs(data_percentual_variation)

# Boxplot for all at once
#install.packages("reshape2")
library(reshape2)
# Melt the dataframe to long format
# Melt the dataframe to long format
data_long <- melt(data_percentual_variation)

# Adjust graphical parameters
par(mar = c(7, 5, 4, 2) + 0.1)  # Increase bottom margin

# Create the boxplot without x-axis labels
{boxplot(value ~ variable, data = data_long,
        main = "Boxplot of Multiple Variables",
        xlab = "", ylab = "Value",
        col = "lightblue", border = "darkblue", xaxt = 'n')

# Add custom x-axis labels at a 45-degree angle
labels <- levels(data_long$variable)
text(x = 1:length(labels), y = par("usr")[3] - 0.5, labels = labels, srt = 45, adj = 1, xpd = TRUE, cex = 0.8)
}

# Reset graphical parameters to default
#par(mar = c(5, 4, 4, 2) + 0.1)
# This one goes in the report, not so much variability in prices even a lot in X vars


```

We plot these four variables to see its evolution through time. It is possible to see that the industrial production Index and the Shipping price Index have an increase tendency while the energy price have a less defined behavior over time with certain peaks in specific years like 2008 and 2022

```{r}
# reset margins

# Reset layout
# Reset margins to default values
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow = c(3, 1))

# dependent variable
data_reduced$category <- as.factor(clean_data$date)
# Set up the plot without drawing the points
{plot(1:length(data_reduced$category), clean_data$shipping, type = "n", xaxt = "n",xlab="date", ylab = "shipping", main = "Evolution of Shipping Prices Index over time")
axis(1, at = 1:length(data_reduced$category), labels = data_reduced$category)
lines(1:length(data_reduced$category), data_reduced$shipping, type = "o", col = "blue", pch = 16)

plot(1:length(data_reduced$category), clean_data$energy, type = "n", xaxt = "n",xlab="date", ylab = "energy", main = "Evolution of Energy Prices Index over time")
axis(1, at = 1:length(data_reduced$category), labels = data_reduced$category)
lines(1:length(data_reduced$category), data_reduced$energy, type = "o", col = "blue", pch = 16)

plot(1:length(data_reduced$category), clean_data$industrial_prod, type = "n", xaxt = "n",xlab="date", ylab = "Industry production", main = "Evolution of Industrial Production Index over time")
axis(1, at = 1:length(data_reduced$category), labels = data_reduced$category)
lines(1:length(data_reduced$category), data_reduced$industrial_prod, type = "o", col = "blue", pch = 16)
}
```

# Log-transformation

We apply a log transformation with the intention of normalize the data as some variables are index that change over time and others area real values in a wide range. Our idea is to reduce the variability of the data, compressing the scale and helping to linearize those relationships between the variables for the interpretability of the results in the models. We apply log transformation to all variables except those that are already percentages: the unemployment rate and the interest rate.

```{r}
# Create new log-transformed variables
data_reduced <- data_reduced %>%
  mutate(across(c(y, industrial_inputs, metals, energy, shipping, fx,
                  industrial_prod, construction_licences_area,finished_constructions,
                  google_trends),
                list(log = ~log(.)), .names = "{.col}_log"))

# Subset the dataframe by selecting specific columns
data_final <- data_reduced %>%
  select(y_log, industrial_inputs_log, metals_log, energy_log,
         shipping_log, fx_log,industrial_prod_log,
         construction_licences_area_log, finished_constructions_log,google_trends_log,
         unemployment, interest_rate)
```

# Covariance and correlation

Now we present the covariance and correlation between the variables of the dataset in order to understand the strength of their linear relation. In particular we show in the following table the result for the correlation of the variables with respect to our variable y (Housing Prices Index):

| Variable                   | Correlation with Housing Prices Index |
|----------------------------|---------------------------------------|
| Industrial_inputs          | 0.1786                                |
| metals                     | 0.2442                                |
| energy                     | -0.0339                               |
| shipping                   | 0.6880                                |
| fx                         | 0.7649                                |
| unemployment               | -0.0114                               |
| industrial_prod            | 0.9447                                |
| interest_rate              | -0.2695                               |
| construction_licences_area | 0.2864                                |
| finished_constructions     | 0.1637                                |
| google_trends              | 0.9318                                |

From the results we see a strong positive relationship between the house price index and the google trend one,a more moderate one with the shipping price index and the fx indicator. While the weaker relationships are found with the energy price index and the unemployment rate. These correlation serve as a point of reference for the linear regression models that we implement in the corresponding section. Also, we found a high positive correlation between the metal price index and the industrial inputs.

```{r}
# Identify columns that do not contain "log"
columns_to_keep <- !grepl("log", names(data_reduced))

# Subset the dataframe to keep only these columns
df_not_log <- data_reduced[, columns_to_keep]

# Remove y_1
#df_not_log <- subset(df_not_log, select = -y_1)

# Display the filtered dataframe
print(df_not_log)
numeric_columns <- select_if(df_not_log, is.numeric)
```


```{r}
cov_matrix <- cov(numeric_columns)
cov_matrix

correl_matrix <- cor(numeric_columns)
correl_matrix
# Heatmap of correl

{image(1:ncol(correl_matrix), 1:ncol(correl_matrix), correl_matrix,
      main = "Correlation Matrix Heatmap",
      xlab = "Variables", ylab = "Variables", axes = FALSE,
      col = heat.colors(20))
axis(1, at = 1:ncol(correl_matrix), labels = colnames(correl_matrix), las = 2)
axis(2, at = 1:ncol(correl_matrix), labels = colnames(correl_matrix), las = 2)}



# Correlation matrix
cor_matrix <- cor(numeric_columns, use = "complete.obs")
print(cor_matrix)

# Visualize the correlation matrix
heatmap(cor_matrix, symm = TRUE)

```

# Multicolinearity Check

Now present a multicollinearity test in top of the results related with the correlation of the variables in relation with the dependent variable of House Price Index. We first calculate the VIF for each predictor variable. We found that some variables like Shipping, Industrial Production Index, Metals Price Index and Industrial Inputs present a high VIF. At this point we consider to remove the Metals Price Index as they show a strong correlation with Industrial Inputs, they are have a similar behaviour and at the end the metal sector is consider on the Indsutrial Inputs Index.

```{r}
# here a value bigger than 10 is supposed to have high multicolinearity 
vif_values <-vif(lm(y_log ~ ., data = data_final))
vif_values
# some variables like shipping, industrial prod, metals and industrial input are high,
# Metals and industrial inputs are very similar / metals is a part of industrial inputs,
# drop metals



data_final <- data_final %>% select(-metals_log)
head(data_final)

# re run multicoliniarity

vif(lm(y_log ~ ., data = data_final))

# Industrial production and unemployment could be related but cant delete one at this point.

# consider regularization using ridge or lasso
# or see how it behaves without industrial inputs

# Assume var_to_exclude is the name of the variable you want to exclude
# var_to_exclude <- "industrial_inputs_log"
# 
# # Create a formula that excludes the specific variable
# predictors <- setdiff(names(data_final), c("y_log", var_to_exclude))
# formula <- as.formula(paste("y_log ~", paste(predictors, collapse = " + ")))
# 
# # Fit the model and calculate VIF
# vif_values <- vif(lm(formula, data = data_final))
# print(vif_values)
```

## Modelling the data

We start with a simple regression model for the data, remember that this process will be executed with the variables after the log normalization. In this first iteration we consider all the explanatory variables.

```{r}
# OLS 1 Log-Log-----------------
# Perform linear regression
model <- lm(y_log ~ ., data = data_final)  
summary(model)
```

In this first model we see a high R-squared, this mean that the model is powerful in terms of explain the variance in the House Price Index, but the model is no completely satisfactory as it includes several predictors without statistical significance, and other with different degree of signifance as industrial production, google trends and interest rate. This can be explain because the R-squared statistic will always increase when more variables are added to the model.

```{r}
# Post estimation Model 1----------------
# See standard R plots
plot(model)
```

After having the model we fit it to evaluate its performance. First we determine the MSE which has a value of 0.001106908 suggesting the the predictions are close to the actual values.

```{r}
# Predict and evaluate the model on the test set
predictions <- predict(model, newdata = data_final)
actuals <- data_final$y_log

# Calculate evaluation metrics
mse <- mean((predictions - actuals)^2)
print(mse)
plot(actuals,predictions)
```

Then we plot the residuals Histogram and QQ plot. The histogram shows an approximately normally distribution centered around zero. However, the QQ plot show some deviations outside the reference line.

```{r}
# checking properties of the residuals

residuals     <- residuals(model)
y.hat <- fitted.values(model)
sum(residuals)
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow = c(2, 1))
# Histogram of residuals
{hist(residuals(model), breaks = 30, main = "Residuals Histogram")
qqnorm(residuals, main = "QQ Plot of Residuals", col ='blue')
qqline(residuals, col = "red")}
```

```{r}
# Do Test of normality to residuals

# For all these tests, the null hypothesis H0  is that the residuals are normally distributed.
# pvalue > 0.05 -> Fail to reject the null hypothesis, suggesting that the residuals are normally distributed.
# Perform Shapiro-Wilk test
shapiro_test <- shapiro.test(residuals)
print(shapiro_test)

# Perform Kolmogorov-Smirnov test
ks_test <- ks.test(residuals, "pnorm", mean = mean(residuals), sd = sd(residuals))
print(ks_test)

# Both p-values are > 0.05, so the residuals should be normal


# Multicolinearity

vif(model)
# Shipping and Industrial production could be linearly correlated


# Model fit
summary(model)$r.squared


# Influential plots
# Cook's Distance Plot
{plot(cooks.distance(model), main="Cook's Distance")
abline(h = 4/(nrow(data_final)-length(coef(model))), col="red")}  # Common threshold line

# Leverage Plot
{plot(hatvalues(model), main="Leverage Values")
abline(h = 2*mean(hatvalues(model)), col="red")}  # Common threshold line


```

# Drop variables according to their significance

After the first iteration we consider the process of eliminating variables based on its significance to the model. The first variable that we consider to discard is the Energy Price Index, for this we proceed with an ANOVA F-test for the comparison of regression models. As the obtained value is 0.1328 is greater than 0.05 the Energy Price Index is not a significant predictor in the rpesence of the other variables. With this we remove it

```{r}
# Variable Selection-----------
# F-test for the comparison of nested models
full.mod <- lm(y_log ~ ., data = data_final)
summary(full.mod)
# Remove energy
red.mod <- update(full.mod, . ~ . -energy_log)
anova(red.mod, full.mod)
```

We observe again the significance of the variables, and the next no significant variables that we decide to drop were Industrial Inputs Index, Finished Constructions, Construction Licence Area and Unemployment. All of this after eliminating step by step each one following the previous procedure with the ANOVA F-test untill all the predictors were significative.

```{r}
# This value indicates the significance level of the F-statistic.
# A small p-value (typically < 0.05) suggests that the additional predictors
# in the full model significantly improve the model fit compared 
# to the reduced model.

# Energy does not improve model
summary(red.mod)
# Remove industrial_inputs_log (only price variable so far)
red.mod2 <- update(red.mod, . ~ . -industrial_inputs_log)
anova(red.mod, red.mod2)
# Can safely remove industrial inputs (but makes no sense)
summary(red.mod2)
# All predictors are statistically significant

# Remove finished_constructions_log (only price variable so far)
red.mod3 <- update(red.mod2, . ~ . -finished_constructions_log)
anova(red.mod2, red.mod3)
# Can safely remove finished constructions (but makes no sense)
summary(red.mod3)
# All predictors are statistically significant

red.mod4 <- update(red.mod3, . ~ . -construction_licences_area_log)
anova(red.mod3, red.mod4)
# Can safely remove finished constructions (but makes no sense)
summary(red.mod4)

red.mod5 <- update(red.mod4, . ~ . -unemployment)
anova(red.mod4, red.mod5)
# Can safely remove finished constructions (but makes no sense)
summary(red.mod5)
# Post estimation 2--------------
```

The final reduced model continue to show a high R-squared, while the QQ-Plot shows more points outside the reference line. At this point even the fact that now all the variables are significative the model appears to has less explanatory power as the histogram of the residuals show a more skewed behavior even though it is centered in zero.

```{r}
residuals_red     <- residuals(red.mod5)

# Histogram of residuals
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow = c(2, 1))
{hist(residuals(red.mod5), breaks = 30, main = "Residuals Histogram Reduced Model")

# Plot QQ plot and histogram for residuals
qqnorm(residuals_red, main = "QQ Plot of Residuals", col ='blue')
qqline(residuals_red, col = "red")}
# They look approximately normal
```

```{r}
# Plot residuals vs fitted values
{plot(fitted(red.mod5), residuals(red.mod5),
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted Values")
abline(h = 0, col = "red")}
```

```{r}
# Do Test of normality to residuals

# For all these tests, the null hypothesis H0  is that the residuals are normally distributed.
# pvalue > 0.05 -> Fail to reject the null hypothesis, suggesting that the residuals are normally distributed.
# Perform Shapiro-Wilk test
shapiro_test <- shapiro.test(residuals_red)
print(shapiro_test)

# Perform Kolmogorov-Smirnov test
ks_test <- ks.test(residuals_red, "pnorm", mean = mean(residuals_red), sd = sd(residuals_red))
print(ks_test)

# Both p-values are > 0.05, so the residuals should be normal

# Check for Homoskedasticity
{plot(fitted(red.mod5), residuals(red.mod5), main="Residuals vs Fitted")
abline(h=0, col="red")}

# Breusch-Pagan Test

bptest(red.mod5)
# P value < 0.05 -> There is Heteroskedasticiy

# Multicolinearity

vif(red.mod5)
# Shipping and Industrial production could be linearly correlated


# Model fit
summary(red.mod5)$r.squared


# Influential plots
# Cook's Distance Plot
{plot(cooks.distance(red.mod5), main="Cook's Distance")
abline(h = 4/(nrow(data_final)-length(coef(red.mod5))), col="red")}  # Common threshold line

# Leverage Plot
{plot(hatvalues(red.mod5), main="Leverage Values")
abline(h = 2*mean(hatvalues(red.mod5)), col="red")}  # Common threshold line
```

#Correction of the model with lagged value

As we continue tosee some variability in the residuals, we decided to use a lagged version of the dependent variable as an explanatory variable. This can help to account for the lagged effects between the variables.

```{r}
# Homoskedasticity

# 
# # Check for Homoskedasticity
{plot(fitted(red.mod5), residuals(red.mod5), main="Residuals vs Fitted")
abline(h=0, col="red")}

data_final_with_lag <- data_final %>%
  mutate(y_log_lag1 = lag(y_log, n = 1))

data_final_with_lag <- na.omit(data_final_with_lag)

model_lag <- lm(y_log ~ y_log_lag1+fx_log + 
                industrial_prod_log  + 
                google_trends_log+  
                interest_rate, data = data_final_with_lag)  
summary(model_lag)
#shipping_log
```

After include the lagged effect we consider the reduced model showing a high R-squared. Then we proceed to show the histogram of the residuals and their QQ plot. From the histogram it is possible to see that now the residuals show a more

```{r}
residuals_red     <- residuals(model_lag)

# Histogram of residuals
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow = c(2, 1))
{hist(residuals(model_lag), breaks = 30, main = "Residuals Histogram Reduced Model")

# Plot QQ plot and histogram for residuals
qqnorm(residuals_red, main = "QQ Plot of Residuals", col ='blue')
qqline(residuals_red, col = "red")}
```

```{r}
# Plot residuals vs fitted values
{plot(fitted(model_lag), residuals(model_lag),
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted Values")
abline(h = 0, col = "red")}
```

```{r}
# 
# # Breusch-Pagan Test
# 
# bptest(red.mod3)
# # P value < 0.05 -> There is Heteroskedasticiy
# # There seems to be homoskedasticity bc included y_1
# 
# 
# # Autocorrelation plot
# # Durbin Watson
# dwtest(red.mod3)
# # H0 (null hypothesis): There is no correlation among the residuals.
# # p-value < 0.05 => reject H0, so there looks like there is serial correlation in residuals
# 
# # For positive serial correlation, consider adding lags of the dependent and/or independent variable to the model.
# # For negative serial correlation, check to make sure that none of your variables are overdifferenced.
# # For seasonal correlation, consider adding seasonal dummy variables to the model.
# 
# # Check type of autocorrelation
# 
# # Plot ACF of residuals
#acf(residuals_red, main = "ACF of Regression Residuals")
# 
# # Plot PACF of residuals
#pacf(residuals_red, main = "PACF of Regression Residuals")
# 
# # According to these plots, we should include the lagged y in our model
# # to account for the ARIMA nature of the timeseries
# 
# 
# # Outliers
# std_res <- rstandard(red.mod3)
# plot(std_res, main="Standardized Residuals")
# abline(h=c(-2, 2), col="red")
# 
# # There are are 3 values of outliers
# 
# # Model with lagged y-------------
# 
# # Add lag of y to account for autocorrelation and heteroskedasticity
# # Subset the dataframe by selecting specific columns
# data_final <- data_reduced %>%
#   select(y_log, y_1_log, shipping_log, fx_log,
#          construction_licences_area_log,google_trends_log,
#          unemployment, interest_rate)
# 
# 
# model_lagged <- lm(y_log ~ ., data = data_final)  # Replace 'Y' with the dependent variable name
# summary(model_lagged)
# 
# residuals_lagged    <- residuals(model_lagged)
# # Create a histogram with more customization
# hist(residuals_lagged, 
#      main = "Histogram of Residuals", 
#      xlab = "Residuals", 
#      ylab = "Frequency", 
#      col = "skyblue", 
#      border = "white",
#      breaks = 30)  # You can change the number of bins
# 
# 
# # Plot QQ plot and histogram for residuals
# qqnorm(residuals_lagged, main = "QQ Plot of Residuals", col ='blue')
# qqline(residuals_lagged, col = "red")
# # They look approximately normal
# 
# # Do Test of normality to residuals
# 
# # For all these tests, the null hypothesis H0  is that the residuals are normally distributed.
# # pvalue > 0.05 -> Fail to reject the null hypothesis, suggesting that the residuals are normally distributed.
# # Perform Shapiro-Wilk test
# shapiro_test <- shapiro.test(residuals_lagged)
# print(shapiro_test)
# 
# # Perform Kolmogorov-Smirnov test
# ks_test <- ks.test(residuals_lagged, "pnorm", mean = mean(residuals_lagged), sd = sd(residuals_lagged))
# print(ks_test)
# 
# # Both p-values are > 0.05, so the residuals should be normal
# 
# 
# 
# # Multicolinearity
# 
# vif(model_lagged)
# # Shipping and Industrial production could be linearly correlated
# 
# 
# # Model fit
# summary(model_lagged)$r.squared
# 
# 
# # Influential plots
# # Cook's Distance Plot
# plot(cooks.distance(model_lagged), main="Cook's Distance")
# abline(h = 4/(nrow(data_final)-length(coef(model_lagged))), col="red")  # Common threshold line
# 
# # Leverage Plot
# plot(hatvalues(model_lagged), main="Leverage Values")
# abline(h = 2*mean(hatvalues(model_lagged)), col="red")  # Common threshold line
# 
# # Homoskedasticity
# 
# 
# # Check for Homoskedasticity
# plot(fitted(model_lagged), residuals(model_lagged), main="Residuals vs Fitted")
# abline(h=0, col="red")
# 
# # Breusch-Pagan Test
# 
# bptest(model_lagged)
# # P value < 0.05 -> There is Heteroskedasticiy
# 
# 
# 
# # Autocorrelation plot
# # Durbin Watson
# dwtest(model_lagged)
# # H0 (null hypothesis): There is no correlation among the residuals.
# # p-value < 0.05 => reject H0, so there looks like there is serial correlation in residuals
# 
# # For positive serial correlation, consider adding lags of the dependent and/or independent variable to the model.
# # For negative serial correlation, check to make sure that none of your variables are overdifferenced.
# # For seasonal correlation, consider adding seasonal dummy variables to the model.
# 
# # Check type of autocorrelation
# 
# # # Plot ACF of residuals
# # acf(residuals_lagged, main = "ACF of Regression Residuals")
# # 
# # # Plot PACF of residuals
# # pacf(residuals_lagged, main = "PACF of Regression Residuals")
# 
# # According to these plots, we should include the lagged y in our model
# # to account for the ARIMA nature of the timeseries
# 
# 
# # Outliers
# std_res <- rstandard(model_lagged)
# plot(std_res, main="Standardized Residuals")
# abline(h=c(-2, 2), col="red")
```

This process consider the model with robust std errors, we could think in leave it or not as we did not see it on class.

```{r}
# OLS Robust STD Errors--------------

#install.packages("sandwich")
library(sandwich)

data_final <- data_reduced %>%
  select(y_log, shipping_log, fx_log, construction_licences_area_log,
         industrial_prod_log,google_trends_log,
         unemployment, interest_rate)

model <- lm(y_log ~ ., data = data_final)
summary (model)
# Calculate robust standard errors
robust_se <- coeftest(model, vcov = vcovHC(model, type = "HC1"))

# Display coefficients with robust standard errors
print(robust_se)


# Interpretation:
# B0: when all the covariates are 0, then y_log is 0.55,
# which is 1.74
# B1, A 1% increase in the shipping costs,
# means a 0.09% increase in house prices (MoM)

# Coefficients dont change that much

# Postestimation is the same as red.mod3
# Everything is the same, except the coefficients of the LM, which account for SE
```

# Forward AIC Selection

We Implement the forward selection based on the AIC obtained by the scores. The AIC approach allows a model comparison with the intention to find a good balance between the model fit and the complexity. The process of forward selection stars with no predictors and adding one by one. In thes case we have a no significative predictor the Industrial Inputs Index, this happens because with the AIC criterion is possible to obtain non-significant predictors if their inclusion improves the general performance of the model.

```{r}
# Forward AIC Stepwise selection-------------

# Fit the null model (intercept only)
null_model <- lm(y_log ~ 1, data = data_reduced)

# Specify the full model with all potential predictors
full_model <- lm(y_log ~ industrial_inputs_log + shipping_log + fx_log + industrial_prod_log + 
                   construction_licences_area_log + google_trends_log + 
                   unemployment + interest_rate, data = data_reduced)

# Perform forward selection based on AIC
forward_model <- step(null_model, 
                      scope = list(lower = null_model, upper = full_model), 
                      direction = "forward")

# Display the summary of the selected model
summary(forward_model)
```

```{r}
residuals_red     <- residuals(forward_model)

# Histogram of residuals
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow = c(2, 1))
{hist(residuals(forward_model), breaks = 30, main = "Residuals Histogram Reduced Model")

# Plot QQ plot and histogram for residuals
qqnorm(residuals_red, main = "QQ Plot of Residuals", col ='blue')
qqline(residuals_red, col = "red")}
```

# Backward AIC selection

We implement the backward selection based also based in the AIC obtained by the different models. In this case, contrary to forward selection, all the variables left were in the reduced model obtained by leaving only the most significative variables.¿, so for this case the results are the same obtained before.

```{r}
# Backward AIC Stepwise selection-------------

full_model <- lm(y_log ~ industrial_inputs_log+ shipping_log + fx_log + industrial_prod_log + 
                   construction_licences_area_log + google_trends_log + 
                   unemployment + interest_rate, data = data_reduced)

# Perform backward selection based on AIC
backward_model <- step(full_model, 
                       direction = "backward")

# Display the summary of the selected model
summary(backward_model)

# Backward gives construction licences, which is good

# Overall the models converge to this last model (removing by p-value), stepwise...
# So we can do the post estimation of the red.mod3, because is the same,
# but use the coefficients of the model with robust errors for inference
```

# Lasso regresion model

```{r}
# Regularization------------
# Done over full model
# Load the packages
library(glmnet)
library(caret)
library(Metrics)


# Subset the dataframe by selecting specific columns
data_final <- data_reduced %>%
  select(y_log, industrial_inputs_log, metals_log, energy_log,
         shipping_log, fx_log,industrial_prod_log,
         construction_licences_area_log, finished_constructions_log,google_trends_log,
         unemployment, interest_rate)

# Sample data preparation
# Assuming your data frame is `data_final` and your response variable is `y_log`
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(data_final$y_log, p = .8, 
                                  list = FALSE, 
                                  times = 1)
data_train <- data_final[ trainIndex,]
data_test  <- data_final[-trainIndex,]

# Extract predictors and response
X_train <- model.matrix(y_log ~ ., data_train)[, -1]  # Remove the intercept column
y_train <- data_train$y_log

X_test <- model.matrix(y_log ~ ., data_test)[, -1]  # Remove the intercept column
y_test <- data_test$y_log

# Fit linear regression model
lm_model <- lm(y_log ~ ., data = data_train)

# Predict and evaluate linear regression model
lm_predictions <- predict(lm_model, newdata = data_test)
lm_mse <- mse(y_test, lm_predictions)
lm_r2 <- R2(y_test, lm_predictions)
cat("Linear Regression MSE:", lm_mse, "\n")
cat("Linear Regression R2:", lm_r2, "\n")

# Fit LASSO regression model
lasso_model <- cv.glmnet(X_train, y_train, alpha = 1)  # alpha = 1 for LASSO
lasso_best_lambda <- lasso_model$lambda.min
lasso_predictions <- predict(lasso_model, s = lasso_best_lambda, newx = X_test)
lasso_mse <- mse(y_test, lasso_predictions)
lasso_r2 <- R2(y_test, lasso_predictions)
cat("LASSO Regression MSE:", lasso_mse, "\n")
cat("LASSO Regression R2:", lasso_r2, "\n")

# Coefficients at best lambda
lasso_coefficients <- coef(lasso_model, s = lasso_best_lambda)
print(lasso_coefficients)
```

```{r}
lasso_residuals <- y_test - lasso_predictions
# Plot predicted vs actual values
{plot(y_test, lasso_predictions, 
     xlab = "Actual Values", ylab = "Predicted Values", 
     main = "Predicted vs Actual Values")
abline(0, 1, col = "red")  # Add a line y = x for reference
}
```

```{r}
{qqnorm(lasso_residuals)
qqline(lasso_residuals, col = "red")}
```

# Ridge regression model

```{r}
# Fit Ridge regression model
ridge_model <- cv.glmnet(X_train, y_train, alpha = 0)  # alpha = 0 for Ridge
ridge_best_lambda <- ridge_model$lambda.min
ridge_predictions <- predict(ridge_model, s = ridge_best_lambda, newx = X_test)
ridge_mse <- mse(y_test, ridge_predictions)
ridge_r2 <- R2(y_test, ridge_predictions)
cat("Ridge Regression MSE:", ridge_mse, "\n")
cat("Ridge Regression R2:", ridge_r2, "\n")

# Coefficients at best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)
print(ridge_coefficients)

```

```{r}
ridge_residuals <- y_test - ridge_predictions
# Plot predicted vs actual values
{plot(y_test, ridge_predictions, 
     xlab = "Actual Values", ylab = "Predicted Values", 
     main = "Predicted vs Actual Values")
abline(0, 1, col = "red")  # Add a line y = x for reference
}
```

```{r}
{qqnorm(ridge_residuals)
qqline(ridge_residuals, col = "red")}
```

```{r}
results <- data.frame(
  Model = c("Linear Regression", "LASSO Regression", "Ridge Regression"),
  MSE = c(lm_mse, lasso_mse, ridge_mse),
  R2 = c(lm_r2, lasso_r2, ridge_r2)
)
print(results)
```
